{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First learning attempts\n",
    "### using Beethoven dataset\n",
    "* 29 pieces + transpositions across 2 octaves\n",
    "* ~70h of music (2.7h per transposition)\n",
    "* 0.025s resolution (40fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_NOTE_VELOCITY = True\n",
    "\n",
    "# loading data files names\n",
    "import os\n",
    "\n",
    "path = '.\\\\datasets\\\\beethoven\\\\'\n",
    "file_names = os.listdir(path)\n",
    "file_names = list(filter(lambda fn: '.npz' in fn or '.npy' in fn or '.csv' in fn, file_names))\n",
    "assert len(file_names) > 0, 'Data not found'\n",
    "\n",
    "f'Found {len(file_names)} files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data files\n",
    "from midi_numpy.common import read_numpy_midi\n",
    "file_paths = [f'{path}{fn}' for fn in file_names]\n",
    "\n",
    "from random import choice\n",
    "def load_tracks(n):\n",
    "    print('loading tracks')\n",
    "    sampled_file_paths = [choice(file_paths) for _ in range(n)]\n",
    "    return [read_numpy_midi(fp) for fp in sampled_file_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating x and y's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(tracks):\n",
    "    print('creating x/y')\n",
    "    data_x = [t[:-1] for t in tracks]\n",
    "    data_y = [t[1:] for t in tracks]\n",
    "    if IGNORE_NOTE_VELOCITY:\n",
    "        data_x = [dx[:, :128] for dx in data_x]\n",
    "        data_y = [dy[:, :128] for dy in data_y]\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into chunks (sequences of equal length)\n",
    "import numpy as np\n",
    "CHUNK_LENGTH = 200 # equals to 5s at 0.025s frames\n",
    "\n",
    "def chunkify(data_x, data_y, chunks_per_track_mult=10):\n",
    "    print('chunking')\n",
    "    # for each track picks (10*len(dx)//CHUNK_LENGTH+1) random sequences\n",
    "    data_x_y = [\n",
    "        (dx[split_point:split_point + CHUNK_LENGTH], dy[split_point:split_point + CHUNK_LENGTH]) \n",
    "            for dx, dy in zip(data_x, data_y)\n",
    "            for _ in range(chunks_per_track_mult * len(dx) // CHUNK_LENGTH + 1)\n",
    "            for split_point in [np.random.randint(len(dx) - 1)]\n",
    "    ]\n",
    "    return list(zip(*data_x_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad smaller chunks to CHUNK_SIZE\n",
    "def pad_chunk_sequence(chunk, goal_seq):\n",
    "    d_len = goal_seq - chunk.shape[0]\n",
    "    npad = ((0, d_len), (0, 0))\n",
    "    return np.pad(chunk, npad, 'constant')\n",
    "    \n",
    "def padify(data_x, data_y):\n",
    "    print('padding')\n",
    "    data_x_res = [pad_chunk_sequence(chunk, CHUNK_LENGTH) for chunk in data_x]\n",
    "    data_y_res = [pad_chunk_sequence(chunk, CHUNK_LENGTH) for chunk in data_y]\n",
    "    return data_x_res, data_y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of matrices to highier dim matrices\n",
    "def stackify(data_x, data_y):\n",
    "    print('stacking')\n",
    "    data_x_res = np.stack(data_x)\n",
    "    data_y_res = np.stack(data_y)\n",
    "    return data_x_res, data_y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generator\n",
    "def data_gen(batch_size, track_count=25):\n",
    "    # x data shape should be [batch_size, sequence_len, input_dim]\n",
    "    # since training will be in many-to-many mode, y has same shape    \n",
    "    while True:\n",
    "        print('reloading data')\n",
    "        data_x, data_y = None, None\n",
    "        data_x, data_y = stackify(*padify(*chunkify(*create_x_y(load_tracks(track_count)))))\n",
    "        n_samples = len(data_x)\n",
    "        print(f'loaded {n_samples} samples')\n",
    "        for _ in range(10 * n_samples):\n",
    "            indices = np.random.randint(0, n_samples, batch_size)\n",
    "            yield data_x[indices], data_y[indices]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as K\n",
    "\n",
    "INPUT_SIZE = 128 if IGNORE_NOTE_VELOCITY else 256\n",
    "HIDDEN_SIZE = 512\n",
    "OUTPUT_SIZE = INPUT_SIZE\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SEQUENCE_LENGTH = CHUNK_LENGTH\n",
    "\n",
    "INPUT_SHAPE = (None, INPUT_SIZE)\n",
    "# could be INPUT_SHAPE = (SEQUENCE_LENGTH, INPUT_SIZE)\n",
    "# however predicting would have to have same seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.models.Sequential([\n",
    "    K.layers.LSTM(HIDDEN_SIZE, input_shape=INPUT_SHAPE, return_sequences=True),\n",
    "    K.layers.Dense(OUTPUT_SIZE, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['binary_accuracy', K.metrics.FalsePositives(), K.metrics.FalseNegatives()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or load saved model\n",
    "base_path = ''\n",
    "file_name = 'beth_notransp_randchunk_bcr_512_22epochs_90.0m.h5'\n",
    "model = K.models.load_model(base_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre running operations\n",
    "# some stat data accumultors for re-running model\n",
    "from time import time\n",
    "epochs_elapsed = 0\n",
    "minutes_elapsed = 0\n",
    "gen = data_gen(BATCH_SIZE)\n",
    "test_gen = data_gen(BATCH_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "STEPS_PER_EPOCH = 1000\n",
    "start_time = time()\n",
    "\n",
    "model.fit_generator(\n",
    "    gen, \n",
    "    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=test_gen, \n",
    "    validation_steps=100\n",
    ")\n",
    "\n",
    "minutes_elapsed += (time() - start_time) // 60\n",
    "epochs_elapsed += EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = ''\n",
    "keywords = '_'.join(['beth', 'notransp', 'randchunk'])\n",
    "file_name = f'{keywords}_{HIDDEN_SIZE}_{epochs_elapsed}epochs_{minutes_elapsed}m.h5'\n",
    "\n",
    "K.models.save_model(model, base_path + file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
