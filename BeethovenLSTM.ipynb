{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First learning attempts\n",
    "### using Beethoven dataset\n",
    "* 29 pieces + transpositions across 2 octaves\n",
    "* ~70h of music (2.7h per transposition)\n",
    "* 0.025s resolution (40fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Found 29 files'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IGNORE_NOTE_VELOCITY = True\n",
    "\n",
    "# loading data files names\n",
    "import os\n",
    "\n",
    "path = '/content/drive/My Drive/datasets/beethoven/'\n",
    "file_names = os.listdir(path)\n",
    "file_names = list(filter(lambda fn: '.npz' in fn or '.npy' in fn or '.csv' in fn, file_names))\n",
    "assert len(file_names) > 0, 'Data not found'\n",
    "\n",
    "f'Found {len(file_names)} files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data files\n",
    "def read_numpy_midi(input_path):\n",
    "    import numpy as np\n",
    "    from scipy import sparse\n",
    "    if 'csv' in input_path:\n",
    "        return np.loadtxt(input_path, delimiter=\",\", dtype=np.int32)\n",
    "    elif 'npy' in input_path:\n",
    "        return np.load(input_path).astype(np.float32)\n",
    "    elif 'npz' in input_path:\n",
    "        sparse_numpy = sparse.load_npz(input_path)\n",
    "        return sparse_numpy.toarray().astype(np.float32)\n",
    "\n",
    "file_paths = [f'{path}\\\\{fn}' for fn in file_names]\n",
    "tracks = [read_numpy_midi(fp) for fp in file_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating x and y's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [t[:-1] for t in tracks]\n",
    "data_y = [t[1:] for t in tracks]\n",
    "\n",
    "if IGNORE_NOTE_VELOCITY:\n",
    "    data_x = [dx[:, :128] for dx in data_x]\n",
    "    data_y = [dy[:, :128] for dy in data_y]\n",
    "\n",
    "# allow to free mem\n",
    "tracks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2201, 2201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting data into chunks (sequences of equal length)\n",
    "import numpy as np\n",
    "CHUNK_LENGTH = 200 # equals to 5s at 0.025s frames\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "data_x = flatten([np.array_split(dx, len(dx) // CHUNK_LENGTH + 1) for dx in data_x])\n",
    "data_y = flatten([np.array_split(dy, len(dy) // CHUNK_LENGTH + 1) for dy in data_y])\n",
    "\n",
    "len(data_x), len(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad smaller chunks to CHUNK_SIZE\n",
    "def pad_chunk_sequence(chunk, goal_seq):\n",
    "    d_len = goal_seq - chunk.shape[0]\n",
    "    npad = ((0, d_len), (0, 0))\n",
    "    return np.pad(chunk, npad, 'constant')\n",
    "    \n",
    "data_x = [pad_chunk_sequence(chunk, CHUNK_LENGTH) for chunk in data_x]\n",
    "data_y = [pad_chunk_sequence(chunk, CHUNK_LENGTH) for chunk in data_y]\n",
    "\n",
    "# sanity check\n",
    "for chunk in data_x:\n",
    "    assert len(chunk) == CHUNK_LENGTH, 'failed to pad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of matrices to highier dim matrices\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generator\n",
    "def data_gen(batch_size):\n",
    "    # x data shape should be [batch_size, sequence_len, input_dim]\n",
    "    # since training will be in many-to-many mode, y has same shape\n",
    "    n_samples = len(data_x)\n",
    "    while True:\n",
    "        indices = np.random.randint(0, n_samples, batch_size)\n",
    "        yield data_x[indices], data_y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "INPUT_SIZE = 128 if IGNORE_NOTE_VELOCITY else 256\n",
    "HIDDEN_SIZE = 1024\n",
    "OUTPUT_SIZE = INPUT_SIZE\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "SEQUENCE_LENGTH = CHUNK_LENGTH\n",
    "\n",
    "INPUT_SHAPE = (None, INPUT_SIZE)\n",
    "# could be INPUT_SHAPE = (SEQUENCE_LENGTH, INPUT_SIZE)\n",
    "# however predicting would have to have same seq length\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(HIDDEN_SIZE, input_shape=INPUT_SHAPE, return_sequences=True),\n",
    "    keras.layers.Dense(OUTPUT_SIZE, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error', \n",
    "    optimizer='adam', \n",
    "    metrics=['categorical_accuracy']\n",
    ")\n",
    "\n",
    "gen = data_gen(BATCH_SIZE)\n",
    "STEPS_PER_EPOCH = len(data_x) // BATCH_SIZE\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some stat data accumultors for re-running model\n",
    "from time import time\n",
    "epochs_elapsed = 0\n",
    "minutes_elapsed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 1321s 8s/step - loss: 0.0241 - categorical_accuracy: 0.0165\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "model.fit_generator(gen, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS)\n",
    "\n",
    "minutes_elapsed += (time() - start_time) / 60\n",
    "epochs_elapsed += EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/content/drive/My Drive/datasets/'\n",
    "keywords = '_'.join(['beth', 'notransp'])\n",
    "file_name = f'{keywords}_{HIDDEN_SIZE}_{epochs_elapsed}epochs_{minutes_elapsed}m.h5'\n",
    "\n",
    "keras.models.save_model(model, base_path + file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
